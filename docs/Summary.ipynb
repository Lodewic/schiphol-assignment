{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project summary \n",
    "\n",
    "## Project goal\n",
    "\n",
    "![project goals](figures/project_goals.png)\n",
    "\n",
    "Predictive modeling of delays of departing aircraft at Schiphol. We are provided with 2 raw data files of flights and airports.\n",
    "\n",
    "This project assignment is used to,\n",
    "\n",
    "1. Display technical skills\n",
    "2. Understanding and flexibility when handling new datasets\n",
    "3. Make a prediction model of flight delays\n",
    "\n",
    "While the assignment description specifically calls for a prediction model and data understanding, I have focussed mostly on **1.: Display technical skills.**.\n",
    "\n",
    "I have taken the liberty to deviate quite a bit from the assignment and understand that especially requirement **2. Understanding new dataset** is largely skipped\n",
    "in my results. Allow me to motivate why I have made this decision from the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation for not sticking to assignment requirements\n",
    "\n",
    "Because this assignment is a code assessment rather than a true business case I have taken the liberty of deviating from the requirements. My focus was to prove my technical skills\n",
    "more so than my data exploration or modelling skills. I believe that with my current results I can show my technical strengths in a way that differentiates my approach from others.\n",
    "\n",
    "Because of time constraints I had to choose between,\n",
    "\n",
    "1. Spend time focussing on data understanding and creating a good prediction model\n",
    "2. Spend time focussing on what I believe is important project setup\n",
    "\n",
    "\n",
    "![](figures/deviation_motivation.png)\n",
    "\n",
    "I have chosen to spend most time on **2.**. Because I have not developed a deep data understanding I also do not have a great model performance. However, I do have a project setup in which adding a new better model\n",
    "is a breeze which would make this project much more robust in the future when moving to production.\n",
    "\n",
    "### Unoriginal results from previous internal results\n",
    "\n",
    "In this assignment I felt I had a conflict of interest because my team internally already did quite some exploratory analysis of the flights data and the Schiphol API's before this assignment.\n",
    "We have weekly demo's so I am kind of familiar with the datasets and questions they already asked, though I did not write the code for them at the time. Then I noticed I was writing\n",
    "code to reproduce the answers to questions they had already answered, which I don't think would highlight my technical skill.\n",
    "\n",
    "The first steps I took were to look for their notebooks for data understanding and API descriptions, then I realized that any exploratory analyses, external datasets or models\n",
    "would not be my ideas. So I only did basic exploration and modeling, and instead treated this assignment as the start of a new project within a team of data scientists.\n",
    "\n",
    "The first steps I would take for a new project is to apply structure in a way that allows for continuous data science, where we present results to the business as soon as possible\n",
    "even when the results are still in an early phase and development is required to achieve good models.\n",
    "\n",
    "### Go beyond data analysis\n",
    "\n",
    "The question I was asked to answer was to create a prediction model. As a data analysis project I could deep-dive into the data and make a single-use analysis and show\n",
    "that I could make predictions on the dataset I was given, possibly collect data from Schiphol API's to enrich the feature set and improve the results.\n",
    "Traditionally I would do exactly that, with a lot of inspiration from existing examples on Kaggle and blogs.\n",
    "\n",
    "From experience I have noticed how important good project setup can be. An efficient workflow takes time to setup but will benefit a single data scientist, a team of us and especially\n",
    "the business. To display my coding skills I chose to setup a data analysis pipeline with,\n",
    "\n",
    "- Package dependency management\n",
    "- R and Python code use\n",
    "- Reproducible Snakemake pipeline\n",
    "- Cover all standard steps in modelling pipeline\n",
    "\n",
    "From here I can show how easy it is to now add additional models, features from external data, analysis results, any steps you can write in a notebook.\n",
    "\n",
    "### My differentiating strength\n",
    "\n",
    "Having worked together with data scientists in multiple teams both internally and as a consultant I have noticed that you'll find team members who are strong at\n",
    "modelling, visualization, dashboarding, building API's or even efficiently using unit-tests. What I was missing was motivated knowledge of project management where,\n",
    "\n",
    "1. analysis results are reproducible\n",
    "2. data is reusable across different steps in the analysis pipeline \n",
    "3. models are saved from where they can be deployed\n",
    "\n",
    "I have since taken an interest in ensuring reproducibility, traceability of results and an efficient development setup. By efficient development setup I mean\n",
    "that other contributing data scientists can painlessly add their ideas to the project. Now, any new idea can be implemented in a dedicated notebook that is easy to integrate\n",
    "in a larger pipeline. Each notebook under `/scripts` represents an analysis step, relying on previously created data if possible but not necessarily.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "Predicting aircraft delays at an airport is a core problem for the airport that may affect flights scheduling, managing flow of people, departure gate assignments, etc. \n",
    "Because it is a complex problem to predict airplane delays effectively I have created a project workflow in which a data scientist or a team of data scientists can\n",
    "develop quite painlessly. Traditionally I would approach an analysis assignment like this with a handful of notebooks that explore the data in-depth, then use that knowledge\n",
    "to preprocess the specific dataset I was handed and then create a model to prove how good I can make predictions.\n",
    "together and contribute models to an analysis pipeline.\n",
    "\n",
    "\n",
    "The idea is that when you reach this stage quickly, it becomes easy to communicate new results to the business in a way that they can contribute ideas because your results\n",
    "are easily viewable. Combined with the fact that each analysis step is a concise rendered notebook with results that are understandable by both business and development interest.\n",
    "\n",
    "With the current project setup I have built it has become easy to add new models and analysis steps to the pipeline while reusing earlier results, such as calculated features or \n",
    "train/test set splitting. \n",
    "\n",
    "--- Example model notebook ---\n",
    "\n",
    "## What I focused on \n",
    "\n",
    "### File directory setup\n",
    "\n",
    "Have a look at the README on GitHub for a more detailed file directory setup.\n",
    "\n",
    "Steps taken,\n",
    "\n",
    "- Use cookiecutter from drivendata, https://drivendata.github.io/cookiecutter-data-science/\n",
    "- Scripts in the project are notebooks that can be executed with [papermill](https://papermill.readthedocs.io/en/latest/)\n",
    "    * Note how papermill notebooks are similar to parameterized notebooks on DataBricks.\n",
    "- Data is on Google Cloud Storage, with public read-access annd private write-access.\n",
    "- Use `Snakemake` to structure output files, easily use local, remote bucket or even Azure DataLake\n",
    "- Sphinx documentation from files under `/src`  and rendered example notebooks from `/scripts` \n",
    "\n",
    "### Remote data source\n",
    "\n",
    "I used Google Cloud Storage on a personal account and created a storage bucket.\n",
    "All project data including analysis results, rendered notebooks, models, etc. is all in the bucket!\n",
    "\n",
    "\n",
    "Browse the data here: https://console.cloud.google.com/storage/browser/lvt-schiphol-assignment-snakemake/\n",
    "\n",
    "[![gcp storage browser](figures/gcp_fig.png)](https://console.cloud.google.com/storage/browser/lvt-schiphol-assignment-snakemake/)\n",
    "\n",
    "You may need to login to GCP, but you do not need specific project access.\n",
    "\n",
    "Steps taken,\n",
    "\n",
    "- Create storage bucket in new GCP project\n",
    "- Set public read-access and create service-account for write-access\n",
    "- Define `Snakemake` pipeline to manage all output data files and structure\n",
    "    * Manual data changes causes the pipeline to re-run analysis steps because files go out-of-sync\n",
    "    * All data on the bucket expected to be created from pipeline, similar to managed datasets on DataLake\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducible analysis pipeline\n",
    "\n",
    "For this I use [Snakemake](https://snakemake.readthedocs.io/en/stable/)! Since my job at TNO I have kept an eye on Snakemake, which is sadly still not working smoothly on Windows.\n",
    "It is simply a Python package for which I think the syntax is easy to read and the output results are sufficient for a project like this.\n",
    "\n",
    "As per snakemake,\n",
    "\n",
    "    The Snakemake workflow management system is a tool to create reproducible and scalable data analyses. Workflows are described via a human readable, Python based language. They can be seamlessly scaled to server, cluster, grid and cloud environments, without the need to modify the workflow definition. Finally, Snakemake workflows can entail a description of required software, which will be automatically deployed to any execution environment.\n",
    "\n",
    "Steps taken,\n",
    "\n",
    "- (!) Fork Snakemake and fix recent bug on Windows, https://github.com/Lodewic/snakemake\n",
    "    * Even in the Docker container we'll install my own version of the package\n",
    "- Create Snakefile with pipeline definition\n",
    "- Add config.yml with relevant settings\n",
    "- Authenticate with Google Cloud storage to sync pipeline output\n",
    "- Create rules for each script under `/scripts`\n",
    "- Create multiple conda environment.yml files for different types of scripts\n",
    "\n",
    "Additionally you can find more output here,\n",
    "\n",
    "- [DAG](figures/dag.svg)\n",
    "- [Rulegraph](figures/rulegraph.svg)\n",
    "- [Filegraph](figures/filegraph.svg)\n",
    "- [Report](figures/report.html)\n",
    "\n",
    "The Snakemake pipeline currently looks as follows,\n",
    "\n",
    "[![dag](figures/dag.svg)](figures/dag.svg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Sphinx documentation\n",
    "\n",
    "You can find the full documentation here, https://schiphol-assignment.readthedocs.io/en/latest/\n",
    "\n",
    "[![readthedocs screenshot](figures/readthedocs_screenshot.png)](https://schiphol-assignment.readthedocs.io/en/latest/)\n",
    "\n",
    "Note that I have played with Sphinx earlier, but never to a stage of published pages.\n",
    "This will document the project progress, links to relevant files and most importantly code documentation.\n",
    "\n",
    "Steps taken,\n",
    "\n",
    "- Setup Sphinx project under `/docs`\n",
    "- Set content and config of documentation pages, including rendered notebooks\n",
    "- Setup build pipeline from GitHub on readthedocs.io\n",
    "- Publish pages whenever master branch changes\n",
    "\n",
    "You'll find that the `/docs` directory is fully functional and will allow you to create a basic documentation page. Cool thing is that notebooks are also included\n",
    "in the documentation pages, which then easily integrates our notebooks we use as scripts under `/scripts`. These script notebooks all expect a certain format,\n",
    "namely a description of the notebook, the input parameters and the expected output files.\n",
    "\n",
    "Sphinx docs will include both the python modules under `/src`  and the applied notebooks under `/scripts`. \n",
    "\n",
    "## What I left out\n",
    "\n",
    "Due to time constraints I have not focussed on the data as much for the assignment, as stated earlier. So what you may be missing is,\n",
    "\n",
    "- In-depth data analysis\n",
    "- Pretty plots\n",
    "- Display of object-oriented programming\n",
    "- Connecting to external datasets\n",
    "- Complex time-series models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas profiling\n",
    "\n",
    "A great tool for a quick display of the data as first step when receiving the data.\n",
    "\n",
    "Creating these profiling reports is an easy first step, and the notebooks that create them are a good\n",
    "first example of using notebooks as papermill scripts. \n",
    "\n",
    "[Pandas profiling script](explore__pandas_profiling.ipynb)\n",
    "\n",
    "Output HTML files are saved to our remote bucket and is publicly available,\n",
    "\n",
    "#### Flights data\n",
    "\n",
    "First thing we made was a profiling report of the data for quick results. Luckily the data is already fairly clean from the start.\n",
    "[Flights data profiling report HTML](https://storage.cloud.google.com/lvt-schiphol-assignment-snakemake/reports/profiling_reports/flights.html)\n",
    "\n",
    "[![flights_pp_screenshot](figures/flights_pp_screenshot.png)](https://storage.cloud.google.com/lvt-schiphol-assignment-snakemake/reports/profiling_reports/flights.html)\n",
    "\n",
    "This summary of the data already teaches us a few things,\n",
    "\n",
    "- Missing values of `actualOffBlockTime` where we cannot calculate the delay\n",
    "- Duplicated ID values in the raw data!\n",
    "- 97.2% of all flights is `serviceType`==J , passenger flights\n",
    "- Some columns contain no data at all\n",
    "\n",
    "There are columns which we should not use for delay prediction which I believe to only be known After the delay is known or a flight is even canceled.\n",
    "\n",
    "- expectedTimeBoarding, expectedTimeGateClosing, expectedTimeGateOpen\n",
    "    - No idea when these values are determined, but unlikely that these values are known/equal 2-hours before the scheduled flight\n",
    "    - Therefore these columns not considered for prediction now\n",
    "\n",
    "We will consider these findings downstream when we process the data for model input. Scripts to perform preprocessing are documented themselves so\n",
    "for the implementation of handling the data please look at the scripts.\n",
    "\n",
    "More points to take with us from here,\n",
    "\n",
    "- ID's are unique despite duplicates in the flights data\n",
    "- By far the most flights are of `serviceType` J for Passengers\n",
    "- A lot of airlines only have a handful of flights\n",
    "- A lot of destinations only have a handful of flights\n",
    "\n",
    "- Creating time-series data without data leakage will be a challenge!\n",
    "    * For every flight, we know after the fact what the delay was. Be careful not to take the actual delay when at the time of prediction a plane is simply 'still delayed' but it may be unkown for how long.\n",
    "\n",
    "#### Airports data\n",
    "\n",
    "- [Airports data profiling report HTML](https://storage.cloud.google.com/lvt-schiphol-assignment-snakemake/reports/profiling_reports/airports.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trelliscope display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- link to trelliscope -- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
